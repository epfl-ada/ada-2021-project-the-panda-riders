{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "[Small description of all the imports we do]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from util.dataloader import *\n",
    "from util.finance import *\n",
    "from util.plots import *\n",
    "from util.finance import stock, compare\n",
    "from util.quotebankexploration import *\n",
    "from util.wikipedia import *\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from util.apple_stores import *\n",
    "\n",
    "from task1 import *\n",
    "from task2 import *\n",
    "from task3 import *\n",
    "from task4 import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"TOC\"></a> Table of Contents\n",
    "\n",
    "***\n",
    "\n",
    "* 0. [Introduction](#intro)\n",
    "* 1. [Loading the datasets](#sect_1)\n",
    "* 2. [First look into the Apple stock market and related quotes](#sect_2)\n",
    "* 3. [Impact of the number of pageviews on the speakers' Wikipedia page](#sect_3)\n",
    "    * 3.1 [Wiki labels and wiki ID](#sect_3_1)\n",
    "    * 3.2 [Number of wiki page views](#sect_3_2)\n",
    "    * 3.3 [Exact label for each quotes](#sect_3_3)\n",
    "    * 3.4 [Scoring quotes](#sect_3_4)\n",
    "    * 3.5 [Get back sentiment analysis](#sect_3_5)\n",
    "    * 3.6 [Final plot](#sect_3_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "***\n",
    "\n",
    "This notebook is intended to show and demonstrate the thought process that went into this project, on identifying patterns and correlations in stock markets and media quotes. In particular we have focused on the Apple Stock market, as it is one of the most valuable company on earth and it is widely covered in the media. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the datasets <a class=\"anchor\" id=\"sect_1\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "***\n",
    "\n",
    "In this project we have studied three datasets to provide more insights in the stock price evolution, the coverage of Apple in the media, and its relationship with the various speakers. This first session is dedicated to loading the various datasets and preprocessing the valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple stock : yFinance API\n",
    "The yFinance API is provided by Yahoo Finance and provides an easy access to various financial metrics for most stocks in the market. The _ticker_ of the Apple stock is denoted _AAPL_, and we will first focus on a date range from 2010 up to 2020. We also provide an additional indicator of the daily volatility with the ``Liquidity`` field, which is the daily volume multiplied by the average daily stock price. This indicator in dollars is appropriate to have a quick overview on the quantity of Apple stock traded in a day, as a day of high liquidity is also said to be a day of high volatility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using yFinance API we load various metrics of the Apple stock ranging from 2008 to 2020\n",
    "stock = load_stock(\"AAPL\", 2015, 2020)\n",
    "\n",
    "# We set a day of high volatility as a day among the highest 2% of liquidity in that year.\n",
    "stock = high_volatility(stock, quantile = 0.98)\n",
    "\n",
    "display(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quotebank Dataset\n",
    "The Quotebank data is a large text corpus of more than 178 millions quotations scrapped over 337 websites. As we are focusing on Apple related quotations, we have applied a various amount of filtering to reduce the number of quotes to 310'816. Some of the various techniques employed: \n",
    "1. White list of words that should be contained in quotes : _Apple_, _iPhone_, _Macbook_ etc.\n",
    "2. Black list of words that should not be contained : _Mac n Cheese, _apple_, _Big Apple_ etc.\n",
    "3. White list of speakers, as we also included speakers related to Apple regardless of the aforementioned white words : _Steve Jobs_, _Tim Cook_, _Steve Wozniak_ etc.\n",
    "\n",
    "This final dataset of Apple related quotes have been saved in a `pkl` file to improve the ease of manipulation, and can be accessed with the `get_filtered_quotes` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = get_filtered_quotes()\n",
    "\n",
    "display(quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. First look into the Apple stock market and related quotes <a class=\"anchor\" id=\"sect_2\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_liquidity(stock, quantile=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stock_price(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_quotes(quotes[quotes.date.dt.year >= 2015], quantile = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_analysis(stock[stock.Date.dt.year.isin([2018,2019])], column=\"Liquidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_with_quotes(stock,quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_stock_quotes(stock[stock.Date.dt.year.isin(range(2015,2019))],quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Impact of the number of pageviews on the speakers' Wikipedia page <a class=\"anchor\" id=\"sect_3\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "***\n",
    "\n",
    "Now we have looked the positiveness of the different quotes of our data set, we want to add more depth about on the impact of the quotes on the stock market. To do so, we pose to ourselves the following question: What is the impact of a quote on others ? The response is relatively simple, it depends on how well known the speaker was at the time he or she was quoted in the media. And a quite simple way to have that indicators is the number of pageviews on then speakers' Wikipedia page (if there is one !).\n",
    "\n",
    "#### 3.1 Wiki labels and wiki ID <a class=\"anchor\" id=\"sect_3_1\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "But before accessing to this number of pageviews, we need the exact label of the speakers' Wikipedia page. And for that, we will load the following data set that particularly contains the precise label's page for its ocrresponding wiki ID that we have for each quotes in our quotes' data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the wiki labels with its corresponding wiki ID\n",
    "wiki_labels = get_wiki_labels()[['id', 'label']]\n",
    "display(wiki_labels.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Number of wiki page views <a class=\"anchor\" id=\"sect_3_2\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "After that, we need a way to get the number of pageviews of some speaker for a specific year. For that we use the package `pageviewapi` that gives us all the wiki page views from 2015. We would have liked to have all the pageviews since 2008, but it was too complicated and one person cerate a way to get these data, but the website was not able to work anymore. Then, we had focus our study between 2015 and 2020, and we had design a fucntion that returns the number of pageviews of a specific wiki page and year. This is what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page views for the Steve Jobs wikipedia page in 2015\n",
    "speaker = 'Steve Jobs'\n",
    "year = 2015\n",
    "print(get_pageviews_per_year(speaker, year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Exact label for each quotes <a class=\"anchor\" id=\"sect_3_3\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "The idea now is to get the exact label for each speaker of every single quotes in our quotes data set. This can be done directly by merging the ID in `wiki_labels` with the QID of the data frame `quotes`, but here it's not that simple. The different quotes of our data set given by quotebank has more than one QID because the name of the speaker can be confusing with another speaker, so it gives the two QID in the list for that quote. To deal with this issue, we decided to use look at all the different QID for each quotes, and keep the label that corresponds to the speaker having the maximum number of total pageviews. \n",
    "\n",
    "Before that, we get all the speakers' ID that are given for each quotes in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the speakers' ID\n",
    "speakers_id = get_speakers_ids(quotes)\n",
    "speakers_id_sample = speakers_id.head(5)\n",
    "display(speakers_id_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see that for one speaker for oour quotes, there might have more than one QID. So here is in the following cell the process we applied to get for all speakers the true label in Wikipedia.\n",
    "\n",
    "**Remark :** The following cell show how the process is done over a sample the dataframe `quotes`. We do not do the whole filtering here because the run is about 22h, so we did it once on multiple clusters and save it in a `.pkl` file. The run is so long because of the `pageviewapi` we call at each iteration at least once, to get the number of pageview for the considered QID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right label of the speakers of each quotes.\n",
    "# It adds a new column `label` in quotes dataframe containig the wiki label of the speaker.\n",
    "speakers_labels_sample = find_labels(speakers_id_sample, wiki_labels)\n",
    "display(speakers_labels_sample)\n",
    "\n",
    "# Get the whole data set from a .pkl file\n",
    "speakers_labels = get_speakers_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Scoring quotes <a class=\"anchor\" id=\"sect_3_4\"></a>\n",
    "[Back to table of contents](#TOC)\n",
    "\n",
    "Now we have the exact label of the spaker of each quotes, we want to get the number of the speaker's wiki page views at the year where the quote was published. So the idea is simple: we just take the label and the year of each quotes and use the function `get_pageviews_per_year` to add a new column `score` in our data frame.\n",
    "\n",
    "**Remark :** As in the previous subsection, here is an example ofhow the process works on a small smaple of the data set because is too long (around 8h for this one). Since the large runnning time, we decided to split the steps such taht we can run it on multiple computer. The idea was then, to get pageviews for each speaker for every year between 2015 and 2020. Here is a sample of how the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the process on a small sample\n",
    "speakers_pageviews_sample = get_speakers_pageviews_per_year(speakers_labels_sample)\n",
    "display(speakers_pageviews_sample)\n",
    "\n",
    "# Load the whole data set from a pickle file\n",
    "speakers_pageviews = get_speakers_pageviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we directly join the year of the quotes with the corresponding number of pageviews of the speakers. After that we normalize the value to be sure to have a score that is relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add score column fro our data frame\n",
    "quotes_score_sample = get_score_quotes(quotes, speakers_pageviews)\n",
    "display(quotes_score_sample.head(5))\n",
    "\n",
    "# Get the whole data set from a .pkl file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Get back sentiment analysis <a class=\"anchor\" id=\"sect_3_5\"></a>\n",
    "[Back to table of contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the code of Camille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Final plot <a class=\"anchor\" id=\"sect_3_6\"></a>\n",
    "[Back to table of contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 5. Building a model for stock market prediction\n",
    "Using the quotes related to Apple, speakers data and past stock performance, we can perform a first attempt at predicting the daily stock price and the liquidity. For this section we will use the Facebook's Prophet library, which provides powerful and easy to use forecasting tools. At its core, the model is a modular linear regression model, that can take into account past performance and additional factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_sentiment = pd.read_pickle(\"data/quotes_score.pkl\")\n",
    "\n",
    "prediction_frame = build_prediction_frame(stock[stock.Date.dt.year.isin(range(2015,2018))],quotes_sentiment)\n",
    "\n",
    "m = fit_prophet(Prophet(changepoint_prior_scale=0.05, seasonality_prior_scale=0.1), prediction_frame, features=['positive','negative','total'], response='Open')\n",
    "pred = predict_future(m,prediction_frame,feature_frame=quotes_sentiment)\n",
    "\n",
    "plot_plotly(m,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(stock, quotes_sentiment,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation\n",
    "df_cv = cross_validation(m, initial='150 days', period='30 days', horizon = '60 days',parallel=\"processes\")\n",
    "df_p = performance_metrics(df_cv)\n",
    "\n",
    "print(\"Mean absolute percentage error in a first week horizon\", df_p[\"mape\"].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_series_predict(stock, quotes_sentiment, features = None, response = 'Open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {  \n",
    "  'changepoint_prior_scale': np.linspace(0.2,2,10) ,\n",
    "  'seasonality_prior_scale': np.logspace(-2,1,10),\n",
    "}\n",
    "\n",
    "tuning_results = prophet_cross_validation(param_grid, stock, quotes_sentiment, features = ['positive','negative','total'], response = 'Open', metric = 'mape')\n",
    "tuning_results[tuning_results.mape == tuning_results.mape.min()]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c74d7e7d97113514de40e96121eb5ba24204156bae1740c3c15b4c118766a645"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ada': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
